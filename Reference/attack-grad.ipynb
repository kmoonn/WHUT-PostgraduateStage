{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 加载cifar10数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3, 32, 32)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "# import paddle # 导入paddle库\n",
    "# import paddle.fluid as fluid\n",
    "# from paddle.vision.datasets import Cifar10\n",
    "# from paddle.vision.transforms import Compose, Normalize,ToTensor,ColorJitter,RandomHorizontalFlip\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image  \n",
    "\n",
    "# # data_format='HWC'\n",
    "# transform = Compose([ColorJitter(),RandomHorizontalFlip(),ToTensor()])\n",
    "\n",
    "# cifar10 = Cifar10(mode='train', download=False,data_file =\"data/data46154/cifar-10.tar.gz\", transform=transform)\n",
    "# train_loader = paddle.io.DataLoader(cifar10, batch_size=32, shuffle=True)\n",
    "\n",
    "# cifar10 = Cifar10(mode='test', download=False,data_file =\"data/data46154/cifar-10.tar.gz\",transform=ToTensor())\n",
    "# test_loader = paddle.io.DataLoader(cifar10, batch_size=32, shuffle=True)\n",
    "\n",
    "# for batch_id, data in enumerate(train_loader()):\n",
    "#     x_data = data[0]\n",
    "#     y_data = data[1]\n",
    "#     break\n",
    "\n",
    "# print(x_data.numpy().shape)\n",
    "# print(y_data.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn as nn\n",
    "import paddle\n",
    "\n",
    "class MyVGG16(nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyVGG16, self).__init__()\n",
    "\n",
    "        self.client = nn.Sequential(\n",
    "            nn.Conv2D(3, 64, kernel_size=[3, 3], padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2D(64, 64, kernel_size=[3, 3], padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.serve = nn.Sequential(\n",
    "            nn.MaxPool2D(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Conv2D(64, 128, kernel_size=[3, 3], padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2D(128, 128, kernel_size=[3, 3], padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2D(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Conv2D(128, 256, kernel_size=[3, 3], padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2D(256, 256, kernel_size=[3, 3], padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2D(256, 256, kernel_size=[3, 3], padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2D(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Conv2D(256, 512, kernel_size=[3, 3], padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2D(512, 512, kernel_size=[3, 3], padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2D(512, 512, kernel_size=[3, 3], padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2D(kernel_size=2, stride=2, padding=0),\n",
    "            nn.Conv2D(512, 512, kernel_size=[3, 3], padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2D(512, 512, kernel_size=[3, 3], padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2D(512, 512, kernel_size=[3, 3], padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2D(kernel_size=2, stride=2, padding=0),\n",
    "        )\n",
    "\n",
    "        self.Adaptive = nn.AdaptiveAvgPool2D(output_size=(7, 7))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=25088, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=4096, out_features=10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x,flag = False):\n",
    "\n",
    "        x = self.client(x)\n",
    "\n",
    "        feature = x\n",
    "\n",
    "        if flag == True:\n",
    "            return feature\n",
    "\n",
    "        x = self.serve(x)\n",
    "\n",
    "        x = self.Adaptive(x)\n",
    "\n",
    "        x = paddle.flatten(x, start_axis=1,stop_axis=-1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return feature,x\n",
    "\n",
    "# myModel = MyVGG16()\n",
    "# my = paddle.Model(myModel)\n",
    "# my.summary((1,3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "import paddle.nn.functional as F\n",
    "from paddle.fluid.dygraph import to_variable, save_dygraph\n",
    "from paddle.vision.models import vgg19,vgg16\n",
    "import numpy as np\n",
    "\n",
    "def train(epoch_num = 1000,use_gpu = True, load_model = True,dataSet=\"MNIST\",meath=\"Mosaic\",modelName=\"MLP\"):\n",
    "\n",
    "    place = paddle.CUDAPlace(0) if use_gpu else paddle.CPUPlace()\n",
    "\n",
    "    # 多级目录\n",
    "    savePath = os.path.join(dataSet+\"_\"+modelName,meath)\n",
    "    \n",
    "    if not os.path.exists(savePath):\n",
    "        os.makedirs(savePath)\n",
    "\n",
    "    with paddle.fluid.dygraph.guard(place):\n",
    "        # 实例化模型\n",
    "        if modelName==\"MyNet\":\n",
    "            model = MyNet(num_classes=10)\n",
    "        elif modelName==\"vgg16\":\n",
    "            model = MyVGG16()\n",
    "        elif modelName==\"vgg19\":\n",
    "            # features = make_layers(cfgs[modelName])\n",
    "            # model = VGG(features,num_classes=10)\n",
    "\n",
    "            # model = vgg19(num_classes=10)\n",
    "            # model = VGG(\"VGG\", num_classes=10,layer=19)\n",
    "            model = VGG(vgg19(num_classes = 10))\n",
    " \n",
    "        # 配置优化器\n",
    "        scheduler = paddle.optimizer.lr.ExponentialDecay(learning_rate=0.01, gamma=0.1, verbose=True)\n",
    "        opt = paddle.optimizer.Momentum(learning_rate=scheduler,parameters=model.parameters())\n",
    "        if load_model and os.path.exists(savePath):\n",
    "            model_para, model_opt = paddle.fluid.load_dygraph(savePath+\"/\"+modelName)\n",
    "            model.load_dict(model_para)\n",
    "            # opt.load_dict(model_opt)\n",
    "\n",
    "            # cnt = 0\n",
    "            # for name, value in model.named_parameters():\n",
    "            #     # if cnt < 17:\n",
    "            #     if cnt < 7:\n",
    "            #     # if name[:2] != \"fc\":\n",
    "            #         value.stop_gradient = True\n",
    "            #         cnt = cnt+1\n",
    "        # 模型训练\n",
    "        model.train()\n",
    "        print('Start training...')\n",
    "        for epoch in range(epoch_num):\n",
    "            for batch_id, data in enumerate(train_loader()):\n",
    "                x_data = data[0]\n",
    "                y_data = paddle.to_tensor(data[1])\n",
    "                y_data = paddle.unsqueeze(y_data, 1)\n",
    "\n",
    "                _,pre = model(x_data)\n",
    "                avg_loss = F.cross_entropy(input=pre, label=y_data)\n",
    "\n",
    "                if batch_id % 100 == 0:\n",
    "                    print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, avg_loss.numpy()))\n",
    "\n",
    "                avg_loss.backward()\n",
    "                opt.step()\n",
    "                opt.clear_grad()\n",
    "            \n",
    "            if (epoch+1) % 20 == 0:\n",
    "                scheduler.step()\n",
    "\n",
    "            if (epoch+1) % 10 ==0:\n",
    "                    save_dygraph(model.state_dict(),os.path.join(savePath,modelName))\n",
    "                    # save_dygraph(opt.state_dict(),os.path.join(savePath,modelName))\n",
    "\n",
    "            # 模型验证\n",
    "            model.eval()\n",
    "            accuracies = []\n",
    "            losses = []\n",
    "            for batch_id, data in enumerate(test_loader()):\n",
    "                x_data = data[0]\n",
    "                y_data = paddle.to_tensor(data[1])\n",
    "                y_data = paddle.unsqueeze(y_data, 1)\n",
    "\n",
    "                _,pre = model(x_data)\n",
    "                # 获取预测结果\n",
    "                loss = F.cross_entropy(pre, y_data)\n",
    "                acc = paddle.metric.accuracy(pre, y_data)\n",
    "                accuracies.append(acc.numpy())\n",
    "                losses.append(loss.numpy())\n",
    "\n",
    "            avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "            print(\"[Test] accuracy/loss: {}/{}\".format(avg_acc, avg_loss))\n",
    "            model.train()\n",
    "            \n",
    "        # 模型保存\n",
    "        # save_dygraph(opt.state_dict(), os.path.join(savePath,modelName))\n",
    "        save_dygraph(model.state_dict(),os.path.join(savePath,modelName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train(epoch_num=100,load_model=False,dataSet=\"Cifar10\",meath=\"no\",modelName=\"vgg16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import paddle # 导入paddle库\n",
    "# import paddle.fluid as fluid\n",
    "# from paddle.vision.datasets import Cifar10\n",
    "# from paddle.vision.transforms import Compose, Normalize,ToTensor\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image  \n",
    "\n",
    "# # data_format='HWC'\n",
    "# transform = Compose([ToTensor()])\n",
    "\n",
    "# cifar10 = Cifar10(mode='test', download=False,data_file =\"data/data46154/cifar-10.tar.gz\",transform=transform)\n",
    "# test_loader = paddle.io.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
    "\n",
    "# cifar10 = Cifar10(mode='train', download=False,data_file =\"data/data46154/cifar-10.tar.gz\", transform=transform)\n",
    "# train_loader = paddle.io.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
    "\n",
    "# for batch_id, data in enumerate(train_loader()):\n",
    "#     x_data = data[0]\n",
    "#     y_data = data[1]\n",
    "#     break\n",
    "\n",
    "# print(x_data.numpy().shape)\n",
    "# print(y_data.numpy().shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 加载某一类数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import numpy as np\n",
    "import six\n",
    "from PIL import Image\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import paddle\n",
    "from paddle.io import Dataset\n",
    "\n",
    "MODE_FLAG_MAP = {\n",
    "    'train10': 'data_batch',\n",
    "    'test10': 'test_batch',\n",
    "    'train100': 'train',\n",
    "    'test100': 'test'\n",
    "}\n",
    "\n",
    "class Cifar10(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_file=None,\n",
    "                 mode='train',\n",
    "                 category = 'lable',\n",
    "                 value = 1,\n",
    "                 transform=None,\n",
    "                 backend=None):\n",
    "        assert mode.lower() in ['train', 'test', 'train', 'test'], \\\n",
    "            \"mode should be 'train10', 'test10', 'train100' or 'test100', but got {}\".format(mode)\n",
    "        self.mode = mode.lower()\n",
    "\n",
    "        if backend is None:\n",
    "            backend = paddle.vision.get_image_backend()\n",
    "        if backend not in ['pil', 'cv2']:\n",
    "            raise ValueError(\n",
    "                \"Expected backend are one of ['pil', 'cv2'], but got {}\"\n",
    "                .format(backend))\n",
    "        self.backend = backend\n",
    "\n",
    "        self.category = category\n",
    "        self.value = value\n",
    "\n",
    "        self._init_flag()\n",
    "\n",
    "        self.data_file = data_file\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        # read dataset into memory\n",
    "        self._load_data()\n",
    "\n",
    "        self.dtype = paddle.get_default_dtype()\n",
    "\n",
    "    def _init_flag(self):\n",
    "        self.flag = MODE_FLAG_MAP[self.mode + '10']\n",
    "\n",
    "    def _mse(self,a, b):\n",
    "        mse = np.mean((a/1.0-b/1.0)**2)\n",
    "        # np.sqrt(np.sum((a-b)**2))\n",
    "        # print(mse)\n",
    "        return mse\n",
    "\n",
    "    def _load_data(self):\n",
    "        msedata = None\n",
    "        self.data = []\n",
    "        with tarfile.open(self.data_file, mode='r') as f:\n",
    "            names = (each_item.name for each_item in f\n",
    "                     if self.flag in each_item.name)\n",
    "\n",
    "            names = sorted(list(names))\n",
    "\n",
    "            for name in names:\n",
    "                batch = pickle.load(f.extractfile(name), encoding='bytes')\n",
    "\n",
    "                data = batch[six.b('data')]\n",
    "                labels = batch.get(\n",
    "                    six.b('labels'), batch.get(six.b('fine_labels'), None))\n",
    "                assert labels is not None\n",
    "                # 得到一个文件里的所有数据，train有5个文件，test有一个文件\n",
    "                for sample, label in six.moves.zip(data, labels):\n",
    "                    if self.category == 'lable':\n",
    "                        if label == self.value:\n",
    "                            # print(sample.shape)\n",
    "                            # print(label)\n",
    "                            # if len(self.data) > 200:\n",
    "                            #     break\n",
    "                            self.data.append((sample, label))\n",
    "                            \n",
    "                    elif self.category == 'mse':\n",
    "                        if msedata is None:\n",
    "                            msedata = sample\n",
    "                        if self._mse(sample,msedata) < self.value:\n",
    "                            # print(sample.shape)\n",
    "                            # print(label)\n",
    "                            # print(self._mse(sample,msedata))\n",
    "                            self.data.append((sample, label))\n",
    "        print(len(self.data))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx]\n",
    "        image = np.reshape(image, [3, 32, 32])\n",
    "        image = image.transpose([1, 2, 0])\n",
    "\n",
    "        if self.backend == 'pil':\n",
    "            image = Image.fromarray(image.astype('uint8'))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.backend == 'pil':\n",
    "            return image, np.array(label).astype('int64')\n",
    "\n",
    "        return image.astype(self.dtype), np.array(label).astype('int64')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0217 17:16:24.000757   429 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0217 17:16:24.006695   429 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 3, 32, 32]\n",
      "[3, 32, 32]\n",
      "(32, 3, 32, 32)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "import paddle # 导入paddle库\n",
    "import paddle.fluid as fluid\n",
    "from paddle.vision.transforms import Compose, Normalize,ToTensor\n",
    "\n",
    "transform = Compose([ToTensor()])\n",
    "\n",
    "cifar10 = Cifar10(mode='train',category = 'lable',value = 1,data_file =\"data/data46154/cifar-10.tar.gz\",transform=transform)\n",
    "train_loader = paddle.io.DataLoader(cifar10, batch_size=32, shuffle=True)\n",
    "\n",
    "cifar10 = Cifar10(mode='test',category = 'lable',value = 1,data_file =\"data/data46154/cifar-10.tar.gz\",transform=transform)\n",
    "test_loader = paddle.io.DataLoader(cifar10, batch_size=32, shuffle=True)\n",
    "\n",
    "for batch_id, data in enumerate(test_loader()):\n",
    "    x_data = data[0]\n",
    "    y_data = data[1]\n",
    "    print(x_data.shape)\n",
    "    print(x_data[10].shape)\n",
    "    break\n",
    "\n",
    "print(x_data.numpy().shape)\n",
    "print(y_data.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 组网"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn.functional as F\n",
    "\n",
    "class MyNet(paddle.nn.Layer):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(MyNet, self).__init__()\n",
    "\n",
    "        self.conv1 = paddle.nn.Conv2D(in_channels=3, out_channels=32, kernel_size=(3, 3))\n",
    "        self.pool1 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = paddle.nn.Conv2D(in_channels=32, out_channels=64, kernel_size=(3,3))\n",
    "        self.pool2 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = paddle.nn.Conv2D(in_channels=64, out_channels=64, kernel_size=(3,3))\n",
    "\n",
    "        self.flatten = paddle.nn.Flatten()\n",
    "\n",
    "        self.linear1 = paddle.nn.Linear(in_features=1024, out_features=64)\n",
    "        self.linear2 = paddle.nn.Linear(in_features=64, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# model = MyNet(10)\n",
    "# # model = paddle.Model(model)\n",
    "# params_info = paddle.summary(model, (1, 3, 32, 32))\n",
    "# print(params_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 预训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "epoch: 0, batch_id: 0, loss is: [2.4043493]\n",
      "epoch: 0, batch_id: 100, loss is: [1.7685595]\n",
      "epoch: 0, batch_id: 200, loss is: [1.5871414]\n",
      "epoch: 0, batch_id: 300, loss is: [1.3216443]\n",
      "epoch: 0, batch_id: 400, loss is: [1.5742823]\n",
      "epoch: 0, batch_id: 500, loss is: [1.4237189]\n",
      "epoch: 0, batch_id: 600, loss is: [1.526447]\n",
      "epoch: 0, batch_id: 700, loss is: [1.2738795]\n",
      "[Test] accuracy/loss: 0.5514530539512634/1.2938047647476196\n",
      "epoch: 1, batch_id: 0, loss is: [1.2795875]\n",
      "epoch: 1, batch_id: 100, loss is: [1.2381054]\n",
      "epoch: 1, batch_id: 200, loss is: [1.0957798]\n",
      "epoch: 1, batch_id: 300, loss is: [1.1464214]\n",
      "epoch: 1, batch_id: 400, loss is: [1.0571482]\n",
      "epoch: 1, batch_id: 500, loss is: [1.1929607]\n",
      "epoch: 1, batch_id: 600, loss is: [1.2117125]\n",
      "epoch: 1, batch_id: 700, loss is: [1.0328155]\n",
      "[Test] accuracy/loss: 0.6149482727050781/1.0800285339355469\n",
      "epoch: 2, batch_id: 0, loss is: [1.0642426]\n",
      "epoch: 2, batch_id: 100, loss is: [0.99873406]\n",
      "epoch: 2, batch_id: 200, loss is: [0.93957174]\n",
      "epoch: 2, batch_id: 300, loss is: [0.93785745]\n",
      "epoch: 2, batch_id: 400, loss is: [0.9671942]\n",
      "epoch: 2, batch_id: 500, loss is: [1.0122929]\n",
      "epoch: 2, batch_id: 600, loss is: [1.1159917]\n",
      "epoch: 2, batch_id: 700, loss is: [1.0124117]\n",
      "[Test] accuracy/loss: 0.6471934914588928/1.0070698261260986\n",
      "epoch: 3, batch_id: 0, loss is: [0.89016795]\n",
      "epoch: 3, batch_id: 100, loss is: [0.99228954]\n",
      "epoch: 3, batch_id: 200, loss is: [1.0468856]\n",
      "epoch: 3, batch_id: 300, loss is: [0.9572191]\n",
      "epoch: 3, batch_id: 400, loss is: [0.96646225]\n",
      "epoch: 3, batch_id: 500, loss is: [0.8511788]\n",
      "epoch: 3, batch_id: 600, loss is: [0.7959828]\n",
      "epoch: 3, batch_id: 700, loss is: [0.76314133]\n",
      "[Test] accuracy/loss: 0.6835191249847412/0.9213292002677917\n",
      "epoch: 4, batch_id: 0, loss is: [0.7529724]\n",
      "epoch: 4, batch_id: 100, loss is: [0.7785891]\n",
      "epoch: 4, batch_id: 200, loss is: [0.8894154]\n",
      "epoch: 4, batch_id: 300, loss is: [0.64738536]\n",
      "epoch: 4, batch_id: 400, loss is: [0.85882175]\n",
      "epoch: 4, batch_id: 500, loss is: [0.8601604]\n",
      "epoch: 4, batch_id: 600, loss is: [0.8436926]\n",
      "epoch: 4, batch_id: 700, loss is: [0.74166715]\n",
      "[Test] accuracy/loss: 0.693869411945343/0.9031820893287659\n",
      "epoch: 5, batch_id: 0, loss is: [0.6737652]\n",
      "epoch: 5, batch_id: 100, loss is: [0.6856222]\n",
      "epoch: 5, batch_id: 200, loss is: [0.6341184]\n",
      "epoch: 5, batch_id: 300, loss is: [0.5364839]\n",
      "epoch: 5, batch_id: 400, loss is: [0.74517965]\n",
      "epoch: 5, batch_id: 500, loss is: [0.61542714]\n",
      "epoch: 5, batch_id: 600, loss is: [0.7260927]\n",
      "epoch: 5, batch_id: 700, loss is: [0.6742387]\n",
      "[Test] accuracy/loss: 0.6924760937690735/0.8936880230903625\n",
      "epoch: 6, batch_id: 0, loss is: [0.6180073]\n",
      "epoch: 6, batch_id: 100, loss is: [0.78609926]\n",
      "epoch: 6, batch_id: 200, loss is: [0.7185871]\n",
      "epoch: 6, batch_id: 300, loss is: [0.47358403]\n",
      "epoch: 6, batch_id: 400, loss is: [0.908848]\n",
      "epoch: 6, batch_id: 500, loss is: [0.666175]\n",
      "epoch: 6, batch_id: 600, loss is: [0.6173328]\n",
      "epoch: 6, batch_id: 700, loss is: [0.57028043]\n",
      "[Test] accuracy/loss: 0.7073049545288086/0.853467583656311\n",
      "epoch: 7, batch_id: 0, loss is: [0.6569339]\n",
      "epoch: 7, batch_id: 100, loss is: [0.652088]\n",
      "epoch: 7, batch_id: 200, loss is: [0.7307172]\n",
      "epoch: 7, batch_id: 300, loss is: [0.8691412]\n",
      "epoch: 7, batch_id: 400, loss is: [0.51609266]\n",
      "epoch: 7, batch_id: 500, loss is: [0.762838]\n",
      "epoch: 7, batch_id: 600, loss is: [0.63581073]\n",
      "epoch: 7, batch_id: 700, loss is: [0.6588855]\n",
      "[Test] accuracy/loss: 0.7008360028266907/0.8715171813964844\n",
      "epoch: 8, batch_id: 0, loss is: [0.78347933]\n",
      "epoch: 8, batch_id: 100, loss is: [0.40310937]\n",
      "epoch: 8, batch_id: 200, loss is: [0.5843805]\n",
      "epoch: 8, batch_id: 300, loss is: [0.5042621]\n",
      "epoch: 8, batch_id: 400, loss is: [0.71515656]\n",
      "epoch: 8, batch_id: 500, loss is: [0.55359316]\n",
      "epoch: 8, batch_id: 600, loss is: [0.8297603]\n",
      "epoch: 8, batch_id: 700, loss is: [0.62511456]\n",
      "[Test] accuracy/loss: 0.7034235596656799/0.907012939453125\n",
      "epoch: 9, batch_id: 0, loss is: [0.5201669]\n",
      "epoch: 9, batch_id: 100, loss is: [0.4529673]\n",
      "epoch: 9, batch_id: 200, loss is: [0.55081475]\n",
      "epoch: 9, batch_id: 300, loss is: [0.43071458]\n",
      "epoch: 9, batch_id: 400, loss is: [0.60839933]\n",
      "epoch: 9, batch_id: 500, loss is: [0.6883482]\n",
      "epoch: 9, batch_id: 600, loss is: [0.6165849]\n",
      "epoch: 9, batch_id: 700, loss is: [0.72569937]\n",
      "[Test] accuracy/loss: 0.7212380766868591/0.8569695353507996\n"
     ]
    }
   ],
   "source": [
    "import os, cv2\n",
    "import paddle.nn.functional as F\n",
    "from paddle.fluid.dygraph import to_variable, save_dygraph\n",
    "from paddle.vision.models import vgg19,vgg16\n",
    "import numpy as np\n",
    "\n",
    "def train(epoch_num = 1000,use_gpu = True, load_model = True,dataSet=\"MNIST\",meath=\"Mosaic\",modelName=\"MLP\"):\n",
    "\n",
    "    place = paddle.CUDAPlace(0) if use_gpu else paddle.CPUPlace()\n",
    "    with fluid.dygraph.guard(place):\n",
    "        # 实例化模型\n",
    "        if modelName==\"MyNet\":\n",
    "            model = MyNet(num_classes=10)\n",
    "            # model = vgg19(num_classes=10)\n",
    "\n",
    "        # 配置优化器\n",
    "        opt = paddle.optimizer.Adam(learning_rate=0.001,parameters=model.parameters())\n",
    "        \n",
    "        # 多级目录\n",
    "        savePath = os.path.join(dataSet+\"_\"+modelName,meath)\n",
    "        if load_model and os.path.exists(savePath):\n",
    "            model_para, model_opt = fluid.load_dygraph(savePath)\n",
    "\n",
    "            model.load_dict(model_para)\n",
    "            opt.set_dict(model_opt)\n",
    "        \n",
    "        if not os.path.exists(savePath):\n",
    "            os.makedirs(savePath)\n",
    "\n",
    "        # 模型训练\n",
    "        model.train()\n",
    "        print('Start training...')\n",
    "        for epoch in range(epoch_num):\n",
    "            for batch_id, data in enumerate(train_loader()):\n",
    "                x_data = data[0]\n",
    "                y_data = paddle.to_tensor(data[1])\n",
    "                y_data = paddle.unsqueeze(y_data, 1)\n",
    "\n",
    "                pre = model(x_data)\n",
    "                avg_loss = F.cross_entropy(input=pre, label=y_data)\n",
    "\n",
    "                if batch_id % 100 == 0:\n",
    "                    print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, avg_loss.numpy()))\n",
    "\n",
    "                avg_loss.backward()\n",
    "                opt.step()\n",
    "                opt.clear_grad()\n",
    "\n",
    "                # if epoch%10 ==0:\n",
    "                #     saveName = 'AE{}'.format(epoch)\n",
    "                    # save_dygraph(AE.state_dict(),os.path.join(savePath,saveName))\n",
    "                    # save_dygraph(opt.state_dict(),os.path.join(savePath,saveName))\n",
    "\n",
    "            # 模型验证\n",
    "            model.eval()\n",
    "            accuracies = []\n",
    "            losses = []\n",
    "            for batch_id, data in enumerate(test_loader()):\n",
    "                x_data = data[0]\n",
    "                y_data = paddle.to_tensor(data[1])\n",
    "                y_data = paddle.unsqueeze(y_data, 1)\n",
    "\n",
    "                pre = model(x_data)\n",
    "                # 获取预测结果\n",
    "                loss = F.cross_entropy(pre, y_data)\n",
    "                acc = paddle.metric.accuracy(pre, y_data)\n",
    "                accuracies.append(acc.numpy())\n",
    "                losses.append(loss.numpy())\n",
    "\n",
    "                # if batch_id % 100 == 0:\n",
    "                #     print(\"Test_batch_id: {}, loss is: {}, acc is: {}\".format(batch_id, loss.numpy(), acc.numpy()))\n",
    "            \n",
    "            avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "            print(\"[Test] accuracy/loss: {}/{}\".format(avg_acc, avg_loss))\n",
    "            model.train()\n",
    "        \n",
    "        # 模型保存\n",
    "        save_dygraph(opt.state_dict(), os.path.join(savePath,modelName))\n",
    "        save_dygraph(model.state_dict(),os.path.join(savePath,modelName))\n",
    "    \n",
    "train(epoch_num=10,load_model=False,dataSet=\"Cifar10\",meath=\"no\",modelName=\"MyNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 测试预训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94628906\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# d = VGG19(vgg19(num_classes = 10))\n",
    "# model_para, _ = fluid.load_dygraph(\"Cifar10_vgg19/no/vgg19\")\n",
    "\n",
    "d = MyVGG16()\n",
    "model_para, _ = fluid.load_dygraph(\"Cifar10_vgg16/no/vgg16\")\n",
    "\n",
    "# d = MyNet(10)\n",
    "# model_para, _ = fluid.load_dygraph(\"Cifar10_MyNet/no/MyNet\")\n",
    "\n",
    "d.load_dict(model_para)\n",
    "\n",
    "acc = []\n",
    "for batch_id, data in enumerate(test_loader()):\n",
    "    x_data = data[0]\n",
    "    y_data = paddle.to_tensor(data[1])\n",
    "    y_data = paddle.unsqueeze(y_data, 1)\n",
    "    \n",
    "\n",
    "    _,pre = d(x_data)\n",
    "\n",
    "    acc.append(paddle.metric.accuracy(pre, y_data).numpy())\n",
    "\n",
    "print(np.mean(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81347656\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# d = VGG19(vgg19(num_classes = 10))\n",
    "# model_para, _ = fluid.load_dygraph(\"Cifar10_vgg19/no/vgg19\")\n",
    "\n",
    "# d = MyVGG16()\n",
    "# model_para, _ = fluid.load_dygraph(\"Cifar10_vgg16/no/vgg16\")\n",
    "\n",
    "d = MyNet(10)\n",
    "model_para, _ = fluid.load_dygraph(\"Cifar10_MyNet/no/MyNet\")\n",
    "\n",
    "d.load_dict(model_para)\n",
    "\n",
    "acc = []\n",
    "for batch_id, data in enumerate(test_loader()):\n",
    "    x_data = data[0]\n",
    "    y_data = paddle.to_tensor(data[1])\n",
    "    y_data = paddle.unsqueeze(y_data, 1)\n",
    "    \n",
    "\n",
    "    pre = d(x_data)\n",
    "\n",
    "    acc.append(paddle.metric.accuracy(pre, y_data).numpy())\n",
    "\n",
    "print(np.mean(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 重构网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 编码器结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from math import sqrt,log\n",
    "\n",
    "class myObfuscator(nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(myObfuscator, self).__init__()\n",
    "\n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Conv2D(3, 32, kernel_size=[3, 3], stride=2, padding=1, data_format='NCHW'),\n",
    "            nn.BatchNorm2D(num_features=32, momentum=0.9, epsilon=1e-05),\n",
    "            nn.LeakyReLU(),\n",
    "        \n",
    "            nn.Conv2D(32,64, kernel_size=[3, 3], stride=1, padding=\"SAME\", data_format='NCHW'),\n",
    "            nn.BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2D(64,128, kernel_size=[3, 3], stride=2, padding=1, data_format='NCHW'),\n",
    "            nn.BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05),\n",
    "            nn.LeakyReLU(),\n",
    "\n",
    "            nn.Conv2D(128,128, kernel_size=[3, 3], stride=[2, 2], padding=1, data_format='NCHW'),\n",
    "            nn.BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.decode = nn.Sequential(\n",
    "            # nn.Conv2DTranspose(in_channels=128,out_channels=128,kernel_size=3,stride=2, padding=\"VALID\"),\n",
    "            # nn.Tanh(),\n",
    "            nn.Conv2DTranspose(in_channels=128,out_channels=64,kernel_size=2,stride=2, padding=\"VALID\"),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2DTranspose(in_channels=64,out_channels=32,kernel_size=2,stride=2, padding=0),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2DTranspose(in_channels=32,out_channels=3,kernel_size=2,stride=2, padding=0),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    " \n",
    "        x = self.encode(x)\n",
    "        x = self.decode(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# myModel = myObfuscator()\n",
    "# my = paddle.Model(myModel)\n",
    "# my.summary((1,3,32,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## UNet结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle import nn\n",
    "\n",
    "class Encoder(nn.Layer):#下采样：两层卷积，两层归一化，最后池化。\n",
    "    def __init__(self, num_channels, num_filters):\n",
    "        super(Encoder,self).__init__()#继承父类的初始化\n",
    "        self.conv1 = nn.Conv2D(in_channels=num_channels,\n",
    "                              out_channels=num_filters,\n",
    "                              kernel_size=3,#3x3卷积核，步长为1，填充为1，不改变图片尺寸[H W]\n",
    "                              stride=1,\n",
    "                              padding=1)\n",
    "        self.bn1   = nn.BatchNorm(num_filters,act=\"relu\")#归一化，并使用了激活函数\n",
    "        \n",
    "        self.conv2 = nn.Conv2D(in_channels=num_filters,\n",
    "                              out_channels=num_filters,\n",
    "                              kernel_size=3,\n",
    "                              stride=1,\n",
    "                              padding=1)\n",
    "        self.bn2   = nn.BatchNorm(num_filters,act=\"relu\")\n",
    "        \n",
    "        self.pool  = nn.MaxPool2D(kernel_size=2,stride=2,padding=\"SAME\")#池化层，图片尺寸减半[H/2 W/2]\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x_conv = x           #两个输出，灰色 ->\n",
    "        x_pool = self.pool(x)#两个输出，红色 | \n",
    "        return x_conv, x_pool\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Layer):#上采样：一层反卷积，两层卷积层，两层归一化\n",
    "    def __init__(self, num_channels, num_filters):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.up = nn.Conv2DTranspose(in_channels=num_channels,\n",
    "                                    out_channels=num_filters,\n",
    "                                    kernel_size=2,\n",
    "                                    stride=2,\n",
    "                                    padding=0)#图片尺寸变大一倍[2*H 2*W]\n",
    "\n",
    "        self.conv1 = nn.Conv2D(in_channels=num_filters*2,\n",
    "                              out_channels=num_filters,\n",
    "                              kernel_size=3,\n",
    "                              stride=1,\n",
    "                              padding=1)\n",
    "        self.bn1   = nn.BatchNorm(num_filters,act=\"relu\")\n",
    "        \n",
    "        self.conv2 = nn.Conv2D(in_channels=num_filters,\n",
    "                              out_channels=num_filters,\n",
    "                              kernel_size=3,\n",
    "                              stride=1,\n",
    "                              padding=1)\n",
    "        self.bn2   = nn.BatchNorm(num_filters,act=\"relu\")\n",
    "        \n",
    "    def forward(self,input_conv,input_pool):\n",
    "        x = self.up(input_pool)\n",
    "        h_diff = (input_conv.shape[2]-x.shape[2])\n",
    "        w_diff = (input_conv.shape[3]-x.shape[3])\n",
    "        pad = nn.Pad2D(padding=[h_diff//2, h_diff-h_diff//2, w_diff//2, w_diff-w_diff//2])\n",
    "        x = pad(x)                                #以下采样保存的feature map为基准，填充上采样的feature map尺寸\n",
    "        x = paddle.concat(x=[input_conv,x],axis=1)#考虑上下文信息，in_channels扩大两倍\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        return x\n",
    "    \n",
    "class UNet(nn.Layer):\n",
    "    def __init__(self,num_classes=32):\n",
    "        super(UNet,self).__init__()\n",
    "        self.down1 = Encoder(num_channels=  3, num_filters=64) #下采样\n",
    "        self.down2 = Encoder(num_channels= 64, num_filters=128)\n",
    "        self.down3 = Encoder(num_channels=128, num_filters=256)\n",
    "        self.down4 = Encoder(num_channels=256, num_filters=512)\n",
    "        \n",
    "        self.mid_conv1 = nn.Conv2D(512,1024,1)                 #中间层\n",
    "        self.mid_bn1   = nn.BatchNorm(1024,act=\"relu\")\n",
    "        self.mid_conv2 = nn.Conv2D(1024,1024,1)\n",
    "        self.mid_bn2   = nn.BatchNorm(1024,act=\"relu\")\n",
    "\n",
    "        self.up4 = Decoder(1024,512)                           #上采样\n",
    "        self.up3 = Decoder(512,256)\n",
    "        self.up2 = Decoder(256,128)\n",
    "        self.up1 = Decoder(128,64)\n",
    "        \n",
    "        self.last_conv = nn.Conv2D(64,num_classes,1)           #1x1卷积，softmax做分类\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        x1, x = self.down1(inputs)\n",
    "        x2, x = self.down2(x)\n",
    "        x3, x = self.down3(x)\n",
    "        x4, x = self.down4(x)\n",
    "        \n",
    "        x = self.mid_conv1(x)\n",
    "        x = self.mid_bn1(x)\n",
    "        x = self.mid_conv2(x)\n",
    "        x = self.mid_bn2(x)\n",
    "        \n",
    "        x = self.up4(x4, x)\n",
    "        x = self.up3(x3, x)\n",
    "        x = self.up2(x2, x)\n",
    "        x = self.up1(x1, x)\n",
    "        \n",
    "        x = self.last_conv(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "# paddle.summary(UNet(3), (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 多层全连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from math import sqrt,log\n",
    "\n",
    "class FCL(nn.Layer):\n",
    "    def __init__(self,inputs = 3072):\n",
    "        super(FCL, self).__init__()\n",
    "\n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Linear(inputs,1000),\n",
    "            nn.ReLU(),\n",
    "            # nn.LeakyReLU(),\n",
    "            nn.Linear(1000,500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500,1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000,3072),\n",
    "            nn.ReLU() \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    " \n",
    "        x = self.encode(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# myModel = FCL()\n",
    "# my = paddle.Model(myModel)\n",
    "# my.summary((1,3072))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## FCL+AUTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "\n",
    "class FCLAndAutoencoder(nn.Layer):\n",
    "    def __init__(self,inputs = 3072):\n",
    "        super(FCLAndAutoencoder, self).__init__()\n",
    "\n",
    "        self.fc = FCL(inputs = inputs)\n",
    "\n",
    "        self.branch = myObfuscator()\n",
    "\n",
    "    def forward(self, x):\n",
    " \n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = x.reshape((-1,3,32,32))\n",
    "\n",
    "        img1 = x\n",
    "\n",
    "        img2 = self.branch(x)\n",
    "\n",
    "        return img1,img2\n",
    "\n",
    "# myModel = FCLAndAutoencoder()\n",
    "# my = paddle.Model(myModel)\n",
    "# my.summary((1,3*32*32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## FCL+UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "\n",
    "class FCLAndUNet(nn.Layer):\n",
    "    def __init__(self,inputs = 3072):\n",
    "        super(FCLAndUNet, self).__init__()\n",
    "\n",
    "        self.fc = FCL(inputs = inputs)\n",
    "\n",
    "        self.branch = UNet(3)\n",
    "\n",
    "    def forward(self, x):\n",
    " \n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = x.reshape((-1,3,32,32))\n",
    "        \n",
    "        img1 = x\n",
    "\n",
    "        img2 = self.branch(x)\n",
    "\n",
    "        return img1,img2\n",
    "\n",
    "# myModel = FCLAndUNet()\n",
    "# my = paddle.Model(myModel)\n",
    "# my.summary((1,3*32*32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "\n",
    "class FCLAndMerge(nn.Layer):\n",
    "    def __init__(self,inputs = 3072,a = 0.5,b = 0.5):\n",
    "        super(FCLAndMerge, self).__init__()\n",
    "\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "        self.fc = FCL(inputs = inputs)\n",
    "\n",
    "        self.branch1 = myObfuscator()\n",
    "\n",
    "        self.branch2 = UNet(3)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = x.reshape((-1,3,32,32))\n",
    "\n",
    "        img1 = x\n",
    " \n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "\n",
    "        img2 = self.a * out1 + self.b * out2\n",
    "\n",
    "        return img1,img2\n",
    "\n",
    "# myModel = FCLAndMerge(a = 0.3, b =0.7)\n",
    "# my = paddle.Model(myModel)\n",
    "# my.summary((1,3*32*32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os, cv2\n",
    "# from paddle.fluid.dygraph import to_variable, save_dygraph\n",
    "# import numpy as np\n",
    "# import random\n",
    "# import math\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image  \n",
    "\n",
    "# def gradient_closure(optimizer, model, x_trial, input_gradient, label):\n",
    "\n",
    "#     # def closure():\n",
    "#         optimizer.clear_grad()\n",
    "#         model.clear_gradients()\n",
    "#         loss = F.cross_entropy(model(x_trial), label)\n",
    "\n",
    "#         loss.backward()\n",
    "        \n",
    "#         new_gradient = []\n",
    "#         for name, value in model.named_parameters():\n",
    "#             new_gradient.append(value.grad.flatten())\n",
    "\n",
    "#         total_costs = 0\n",
    "#         for i in range(len(new_gradient)):\n",
    "#             total_costs += 1 - F.cosine_similarity(new_gradient[i], input_gradient[i], axis=0)\n",
    "\n",
    "#         total_costs.backward()\n",
    "#         return total_costs\n",
    "\n",
    "#     # return closure\n",
    "\n",
    "# def train(epoch_num = 1000,use_gpu = True,load_model = True,dataSet=\"MNIST\",meath=\"Mosaic\",modelName=\"MLP\"):\n",
    "    \n",
    "#     # 使用GPU\n",
    "#     place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()\n",
    "\n",
    "#     with fluid.dygraph.guard(place):\n",
    "#         # # 实例化模型\n",
    "#         # if modelName==\"attack\":\n",
    "#         #     attack = MLPandAttack(0.3,0.7)\n",
    "#         # elif modelName==\"Reconstruct\":\n",
    "#         #     re_model = Reconstruct()\n",
    "\n",
    "#         model = MyNet(num_classes=10)\n",
    "#         # model_para, model_opt = fluid.load_dygraph('Cifar10_MyNet/no/MyNet')\n",
    "#         # model.load_dict(model_para)\n",
    "#         model.eval()\n",
    "\n",
    "#         # 读取一张图片并计算梯度\n",
    "#         original_gradient = []\n",
    "#         original_img = None\n",
    "#         original_lable = None\n",
    "#         for batch_id, data in enumerate(test_loader()):\n",
    "#             x_data = data[0]\n",
    "#             y_data = data[1]\n",
    "\n",
    "#             b,c,w,h = x_data.shape\n",
    "\n",
    "#             # 将y再包上一层[[64]]\n",
    "#             # y_data = paddle.to_tensor(data[1])\n",
    "#             # y_data = paddle.unsqueeze(y_data, 1)\n",
    "\n",
    "#             # 遍历一个批量,计算单张图片对应的梯度\n",
    "#             for img,lab in zip(x_data.numpy(),y_data.numpy()):\n",
    "#                 # 单张图片也表示成批量的形式\n",
    "#                 original_img = img\n",
    "#                 img = paddle.to_tensor(np.expand_dims(img,axis = 0))\n",
    "#                 original_lable = paddle.to_tensor(np.expand_dims(lab,axis = 0))\n",
    "    \n",
    "#                 # compute original gradient\n",
    "#                 pre = model(img)\n",
    "                \n",
    "#                 avg_loss = F.cross_entropy(input=pre, label=original_lable)\n",
    "\n",
    "#                 avg_loss.backward()\n",
    "        \n",
    "#                 for name, value in model.named_parameters():\n",
    "#                     original_gradient.append(value.grad.detach().flatten())\n",
    "\n",
    "#                 break\n",
    "#             break\n",
    "\n",
    "#         dummy_data = paddle.standard_normal(shape = [3,32,32]) / 255.\n",
    "#         dummy_data = paddle.reshape(dummy_data,[-1,3,32,32])\n",
    "#         dummy_data.stop_gradient = False\n",
    "#         # dummy_data = paddle.randn(gt_data.size()).requires_grad_(True)\n",
    "        \n",
    "#         # 配置优化器\n",
    "#         optimizer = paddle.optimizer.Adam(learning_rate=0.001,parameters=[dummy_data])\n",
    "\n",
    "#         for epoch in range(epoch_num):\n",
    "#             closure = gradient_closure(optimizer = optimizer, model = model, x_trial = dummy_data, input_gradient = original_gradient, label = original_lable)\n",
    "   \n",
    "#             # rec_loss = optimizer.step(closure)\n",
    "#             rec_loss = optimizer.minimize(closure)\n",
    "\n",
    "#             print(closure.numpy())\n",
    "\n",
    "#             plt.imshow(dummy_data.numpy().reshape(3,32,32).transpose(1,2,0))\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train(epoch_num=100,load_model=False,dataSet=\"Cifar10\",meath=\"class_lable\",modelName=\"attack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os, cv2\n",
    "# from paddle.fluid.dygraph import to_variable, save_dygraph\n",
    "# import numpy as np\n",
    "# import random\n",
    "# import math\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image  \n",
    "\n",
    "# def train(epoch_num = 1000,use_gpu = True,load_model = True,dataSet=\"MNIST\",meath=\"Mosaic\",modelName=\"MLP\"):\n",
    "    \n",
    "#     # 使用GPU\n",
    "#     place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()\n",
    "\n",
    "#     with fluid.dygraph.guard(place):\n",
    "#         # 实例化模型\n",
    "#         if modelName==\"attack\":\n",
    "#             attack = myObfuscatorAndUNet(0.3,0.7)\n",
    "#         elif modelName==\"Reconstruct\":\n",
    "#             re_model = Reconstruct()\n",
    "\n",
    "#         model = MyNet(num_classes=10)\n",
    "#         # model_para, model_opt = fluid.load_dygraph('Cifar10_MyNet/no/MyNet')\n",
    "#         # model.load_dict(model_para)\n",
    "#         model.eval()\n",
    "  \n",
    "#         # 配置优化器\n",
    "#         opt = paddle.optimizer.Adam(learning_rate=0.001,parameters=attack.parameters())\n",
    "        \n",
    "#         attack.train()\n",
    "#         # 多级目录\n",
    "#         savePath = os.path.join(dataSet+\"_\"+modelName,meath)\n",
    "#         if load_model and os.path.exists(savePath):\n",
    "#             model_para, model_opt = fluid.load_dygraph(savePath + '/'+modelName)\n",
    "\n",
    "#             attack.load_dict(model_para)\n",
    "#             # opt.set_dict(model_opt)\n",
    "        \n",
    "#         if not os.path.exists(savePath):\n",
    "#             os.makedirs(savePath)\n",
    "\n",
    "#          # 读取一张图片并计算梯度\n",
    "#         # 读取一张图片并计算梯度\n",
    "#         original_gradient = []\n",
    "#         original_img = None\n",
    "#         original_lable = None\n",
    "#         for batch_id, data in enumerate(test_loader()):\n",
    "#             x_data = data[0]\n",
    "#             y_data = data[1]\n",
    "\n",
    "#             b,c,w,h = x_data.shape\n",
    "\n",
    "#             # 将y再包上一层[[64]]\n",
    "#             # y_data = paddle.to_tensor(data[1])\n",
    "#             # y_data = paddle.unsqueeze(y_data, 1)\n",
    "\n",
    "#             # 遍历一个批量,计算单张图片对应的梯度\n",
    "#             for img,lab in zip(x_data.numpy(),y_data.numpy()):\n",
    "#                 # 单张图片也表示成批量的形式\n",
    "                \n",
    "#                 img = paddle.to_tensor(np.expand_dims(img,axis = 0))\n",
    "#                 original_lable = paddle.to_tensor(np.expand_dims(lab,axis = 0))\n",
    "#                 original_img = img\n",
    "\n",
    "#                 # compute original gradient\n",
    "#                 pre = model(img)\n",
    "                \n",
    "#                 avg_loss = F.cross_entropy(input=pre, label=original_lable)\n",
    "\n",
    "#                 avg_loss.backward()\n",
    "        \n",
    "#                 for name, value in model.named_parameters():\n",
    "#                     original_gradient.append(value.grad.detach().flatten())\n",
    "\n",
    "#                 break\n",
    "#             break\n",
    "            \n",
    "#         # for i in :\n",
    "#         #     # 找到一张最为相似的作为初始解\n",
    "        \n",
    "#         # # 迭代试图恢复\n",
    "#         dummy_data = paddle.standard_normal(shape = [3,32,32]) / 255.\n",
    "#         dummy_data = paddle.reshape(dummy_data,[-1,3,32,32])\n",
    "#         # dummy_data.stop_gradient = False\n",
    "#         for epoch in range(epoch_num):\n",
    "\n",
    "#             re_imag = attack(dummy_data)\n",
    "#             pre = model(re_imag)\n",
    "#             loss = F.cross_entropy(input=pre, label=original_lable)\n",
    "            \n",
    "#             new_gradient = []\n",
    "#             for name, value in model.named_parameters():\n",
    "#                 new_gradient.append(value.grad.flatten())\n",
    "\n",
    "#             total_loss = 0\n",
    "#             for i in range(len(new_gradient)):\n",
    "#                 total_loss += F.mse_loss(new_gradient[i], original_gradient[i])\n",
    "#                 print( F.mse_loss(new_gradient[i], original_gradient[i]).numpy())\n",
    "\n",
    "#             total_loss.backward()\n",
    "            \n",
    "#             # opt.step()\n",
    "#             # opt.clear_grad()\n",
    "            \n",
    "#             opt.minimize(total_loss)\n",
    "#             attack.clear_gradients()\n",
    "\n",
    "#             if batch_id % 100 == 0:\n",
    "#                 print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, total_loss.numpy()))\n",
    "\n",
    "#             if (epoch+1) % 10 == 0:\n",
    "#                 save_dygraph(attack.state_dict(),os.path.join(savePath,modelName))\n",
    "\n",
    "#             for img in re_imag.numpy():\n",
    "#                 img = img.transpose(1,2,0)\n",
    "#                 plt.imshow(img)\n",
    "#                 plt.show()\n",
    "#                 break;\n",
    "\n",
    "#         # 模型保存\n",
    "#         # save_dygraph(opt.state_dict(), os.path.join(savePath,modelName))\n",
    "#         save_dygraph(attack.state_dict(),os.path.join(savePath,modelName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train(epoch_num=100,load_model=False,dataSet=\"Cifar10\",meath=\"class_lable\",modelName=\"attack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 记录下采样点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8903 37954 68296 ... 19038 86540 70472]\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# aa = np.random.choice(10000, 5000, replace=False)\n",
    "# np.savetxt(\"maxIndex5000.txt\",aa)\n",
    "\n",
    "# aa = np.random.choice(100000, 5000, replace=False)\n",
    "# np.savetxt(\"randomIndex5000.txt\",aa)\n",
    "\n",
    "# aa = np.loadtxt(\"randomIndex5000.txt\", delimiter=',').astype('int32')\n",
    "# print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51703 30728 42723 ... 95574 33388 33739]\n"
     ]
    }
   ],
   "source": [
    "# aa = np.random.choice(5000, 3072, replace=False)\n",
    "# np.savetxt(\"maxIndex3072.txt\",aa)\n",
    "\n",
    "# aa = np.random.choice(100000, 3072, replace=False)\n",
    "# np.savetxt(\"randomIndex3072.txt\",aa)\n",
    "\n",
    "# aa = np.loadtxt(\"randomIndex3072.txt\", delimiter=',').astype('int32')\n",
    "# print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "                    avg_loss.backward()\n",
    "\n",
    "                    original_gradient = []\n",
    "                    for name, value in model.named_parameters():\n",
    "                        original_gradient.append(value.grad.detach().flatten())\n",
    "\n",
    "                    model.clear_gradients()\n",
    "               \n",
    "                    oneGrd = paddle.concat(original_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "from paddle.fluid.dygraph import to_variable, save_dygraph\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image  \n",
    "\n",
    "def train(epoch_num = 1000,use_gpu = True,load_model = True,sample = \"random\",dataSet=\"MNIST\",meath=\"Mosaic\",modelName=\"MLP\",inputs = 5000,classid = 5):\n",
    "    \n",
    "    if sample != \"random\":\n",
    "        data_index = np.loadtxt(\"maxIndex\"+str(2)+\".txt\", delimiter=',').astype('int32')\n",
    "        data_index = paddle.to_tensor(data_index)\n",
    "    else:\n",
    "        data_index = np.loadtxt(\"randomIndex\"+str(2)+\".txt\", delimiter=',').astype('int32')\n",
    "        data_index = paddle.to_tensor(data_index)\n",
    "\n",
    "    # 使用GPU\n",
    "    place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()\n",
    "\n",
    "    with fluid.dygraph.guard(place):\n",
    "        # 实例化模型\n",
    "        if modelName==\"FCLAndAutoencoder\":\n",
    "            attack = FCLAndAutoencoder(inputs = inputs)\n",
    "        elif modelName==\"FCLAndUNet\":\n",
    "            attack = FCLAndUNet(inputs = inputs)\n",
    "        elif modelName==\"merge\":\n",
    "            attack = FCLAndMerge(inputs = inputs,a = 0.3, b =0.7)\n",
    "\n",
    "        model = MyNet(num_classes=10)\n",
    "        model_para, _ = fluid.load_dygraph('Cifar10_MyNet/no/MyNet')\n",
    "        model.load_dict(model_para)\n",
    "        model.eval()\n",
    "  \n",
    "        # 多级目录\n",
    "        savePath = os.path.join(dataSet+\"_\"+modelName,meath)\n",
    "        if load_model and os.path.exists(savePath):\n",
    "            # model_para, _ = fluid.load_dygraph(savePath + '/'+modelName)\n",
    "            try:\n",
    "                model_para, _ = fluid.load_dygraph(savePath + '/'+modelName+str(classid))\n",
    "            except:\n",
    "                model_para, _ = fluid.load_dygraph(savePath + '/'+modelName+str(5))\n",
    "\n",
    "            attack.load_dict(model_para)\n",
    "            # opt.set_dict(model_opt)\n",
    "        \n",
    "        if not os.path.exists(savePath):\n",
    "            os.makedirs(savePath)\n",
    "        \n",
    "        # 配置优化器\n",
    "        # scheduler = paddle.optimizer.lr.ExponentialDecay(learning_rate=0.01, gamma=0.1, verbose=True)\n",
    "        opt = paddle.optimizer.Adam(learning_rate=0.001,parameters=attack.parameters())\n",
    "\n",
    "        print('Start training...')\n",
    "        attack.train()\n",
    "        for epoch in range(epoch_num):\n",
    "            for batch_id, data in enumerate(train_loader()):\n",
    "                # 一个批量的图片[64,3,32,32]\n",
    "                x_data = data[0]\n",
    "                # 一个批量的标签[64]\n",
    "                y_data = data[1]\n",
    "\n",
    "                b,c,w,h = x_data.shape\n",
    "\n",
    "                # 将y再包上一层[[64]]\n",
    "                # y_data = paddle.to_tensor(data[1])\n",
    "                # y_data = paddle.unsqueeze(y_data, 1)\n",
    "          \n",
    "                batch_grd = []\n",
    "                # 遍历一个批量,计算单张图片对应的梯度\n",
    "                for img,lab in zip(x_data.numpy(),y_data.numpy()):\n",
    "                    # 单张图片也表示成批量的形式\n",
    "                    image = paddle.to_tensor(np.expand_dims(img,axis = 0))\n",
    "                    lable = paddle.to_tensor(np.expand_dims(lab,axis = 0))\n",
    "       \n",
    "                    # compute original gradient\n",
    "                    pre = model(image)\n",
    "                    \n",
    "                    avg_loss = F.cross_entropy(input=pre, label=lable)\n",
    "\n",
    "                    # avg_loss.backward()\n",
    "\n",
    "                    # dy_dx_list = []\n",
    "                    # for value in model.parameters():\n",
    "                    #     dy_dx_list.append(value.grad.detach().flatten())\n",
    "                    # model.clear_gradients()\n",
    "   \n",
    "                    dy_dx = paddle.grad(avg_loss, model.parameters())\n",
    "\n",
    "                    # 梯度+偏置的组合\n",
    "                    dy_dx_list = [ paddle.flatten(x) for i,x in enumerate(dy_dx)]\n",
    "  \n",
    "                    oneGrd = paddle.concat(dy_dx_list)\n",
    "                    model.clear_gradients()\n",
    "                    # print(oneGrd.shape)\n",
    "                    \n",
    "                    if sample != \"random\":\n",
    "                        # 降序排序\n",
    "                        temp_list = []\n",
    "                        oneGrd = paddle.sort(oneGrd,descending = True)\n",
    "                        temp_list.append(oneGrd[:2500])\n",
    "                        temp_list.append(oneGrd[-2500:])\n",
    "                        oneGrd = paddle.concat(temp_list)\n",
    "                    \n",
    "                    oneGrd = paddle.index_select(x=oneGrd, index=data_index)\n",
    "                    # print(len(dy_dx_list))\n",
    "                    # print(oneGrd.shape)\n",
    "\n",
    "                    batch_grd.append(oneGrd)\n",
    "\n",
    "                Ngrd = paddle.stack(batch_grd, axis=0)\n",
    "                # print(len(Nimg))\n",
    "                # print(type(Nimg))\n",
    "                # print(Ngrd.shape)\n",
    "                \n",
    "                # 至此便得到一个批量的《图片,梯度》对训练集，即 Nimg => x_data\n",
    "                # =================================================================================================\n",
    "                # 以下开始训练重构网络\n",
    "                img1,img2 = attack(Ngrd)\n",
    "\n",
    "                total_loss = F.mse_loss(img1, x_data) + F.mse_loss(img2, x_data)\n",
    "       \n",
    "                # avg_loss.backward()\n",
    "                # opt.step()\n",
    "                # opt.clear_grad()\n",
    "\n",
    "                total_loss.backward()\n",
    "                opt.minimize(total_loss)\n",
    "                attack.clear_gradients()\n",
    "\n",
    "                if batch_id % 500 == 0:\n",
    "                    print(\"epoch: {}, loss is: {}\".format(epoch, total_loss.numpy()))\n",
    "\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                save_dygraph(attack.state_dict(),os.path.join(savePath,modelName+str(classid)))\n",
    "                # scheduler.step()\n",
    "\n",
    "            for img in img2.numpy():\n",
    "                img = img.transpose(1,2,0)\n",
    "                plt.imshow(img)\n",
    "                plt.show()\n",
    "                break;\n",
    "\n",
    "        # 模型保存\n",
    "        # save_dygraph(opt.state_dict(), os.path.join(savePath,modelName))\n",
    "        save_dygraph(attack.state_dict(),os.path.join(savePath,modelName+str(classid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train(epoch_num=100,load_model=True,sample = \"random\",dataSet=\"Cifar10\",meath=\"class_lable\",modelName=\"merge\",inputs = 5000,classid = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train(epoch_num=100,load_model=True,sample = \"random\",dataSet=\"Cifar10\",meath=\"class_lable\",modelName=\"FCLAndUNet\",inputs = 5000,classid=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train(epoch_num=40,load_model=True,sample = \"random\",dataSet=\"Cifar10\",meath=\"class_lable\",modelName=\"FCLAndAutoencoder\",inputs = 5000,classid = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 使用w进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "from paddle.fluid.dygraph import to_variable, save_dygraph\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image  \n",
    "\n",
    "def train(epoch_num = 1000,use_gpu = True,load_model = True,sample = \"random\",dataSet=\"MNIST\",meath=\"Mosaic\",modelName=\"MLP\"):\n",
    "    \n",
    "    if sample != \"random\":\n",
    "        data_index = np.loadtxt(\"maxIndex2.txt\", delimiter=',').astype('int32')\n",
    "        data_index = paddle.to_tensor(data_index)\n",
    "    else:\n",
    "        data_index = np.loadtxt(\"randomIndex2.txt\", delimiter=',').astype('int32')\n",
    "        data_index = paddle.to_tensor(data_index)\n",
    "\n",
    "    # 使用GPU\n",
    "    place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()\n",
    "\n",
    "    with fluid.dygraph.guard(place):\n",
    "        # 实例化模型\n",
    "        if modelName==\"attack\":\n",
    "            attack = MLPandAttack(inputs = 5000,a = 0.3, b =0.7)\n",
    "        elif modelName==\"Reconstruct\":\n",
    "            re_model = Reconstruct()\n",
    "\n",
    "        model = MyNet(num_classes=10)\n",
    "        model_para, _ = fluid.load_dygraph('Cifar10_MyNet/no/MyNet')\n",
    "        model.load_dict(model_para)\n",
    "        model.eval()\n",
    "  \n",
    "        # 多级目录\n",
    "        savePath = os.path.join(dataSet+\"_\"+modelName,meath)\n",
    "        if load_model and os.path.exists(savePath):\n",
    "            model_para, _ = fluid.load_dygraph(savePath + '/'+modelName+\"0605\")\n",
    "\n",
    "            attack.load_dict(model_para)\n",
    "            # opt.set_dict(model_opt)\n",
    "        \n",
    "        if not os.path.exists(savePath):\n",
    "            os.makedirs(savePath)\n",
    "        \n",
    "        # 配置优化器\n",
    "        opt = paddle.optimizer.Adam(learning_rate=0.001,parameters=attack.parameters())\n",
    "\n",
    "        print('Start training...')\n",
    "        attack.train()\n",
    "        for epoch in range(epoch_num):\n",
    "            for batch_id, data in enumerate(train_loader()):\n",
    "                # 一个批量的图片[64,3,32,32]\n",
    "                x_data = data[0]\n",
    "                # 一个批量的标签[64]\n",
    "                y_data = data[1]\n",
    "\n",
    "                b,c,w,h = x_data.shape\n",
    "\n",
    "                # 将y再包上一层[[64]]\n",
    "                # y_data = paddle.to_tensor(data[1])\n",
    "                # y_data = paddle.unsqueeze(y_data, 1)\n",
    "          \n",
    "                batch_grd = []\n",
    "                # 遍历一个批量,计算单张图片对应的梯度\n",
    "                for img,lab in zip(x_data.numpy(),y_data.numpy()):\n",
    "                    # 单张图片也表示成批量的形式\n",
    "                    image = paddle.to_tensor(np.expand_dims(img,axis = 0))\n",
    "                    lable = paddle.to_tensor(np.expand_dims(lab,axis = 0))\n",
    "       \n",
    "                    # compute original gradient\n",
    "                    pre = model(image)\n",
    "                    \n",
    "                    avg_loss = F.cross_entropy(input=pre, label=lable)\n",
    "\n",
    "                    # avg_loss.backward()\n",
    "\n",
    "                    # dy_dx_list = []\n",
    "                    # for value in model.parameters():\n",
    "                    #     dy_dx_list.append(value.grad.detach().flatten())\n",
    "                    # model.clear_gradients()\n",
    "   \n",
    "                    dy_dx = paddle.grad(avg_loss, model.parameters())\n",
    "\n",
    "                    # 梯度+偏置的组合\n",
    "                    dy_dx_list = [ paddle.flatten(x) for i,x in enumerate(dy_dx)]\n",
    "  \n",
    "                    oneGrd = paddle.concat(dy_dx_list)\n",
    "                    model.clear_gradients()\n",
    "                    # print(oneGrd.shape)\n",
    "                    \n",
    "                    if sample != \"random\":\n",
    "                        # 降序排序\n",
    "                        temp_list = []\n",
    "                        oneGrd = paddle.sort(oneGrd,descending = True)\n",
    "                        temp_list.append(oneGrd[:2500])\n",
    "                        temp_list.append(oneGrd[-2500:])\n",
    "                        oneGrd = paddle.concat(temp_list)\n",
    "                    \n",
    "                    oneGrd = paddle.index_select(x=oneGrd, index=data_index)\n",
    "                    # print(len(dy_dx_list))\n",
    "                    # print(oneGrd.shape)\n",
    "\n",
    "                    batch_grd.append(oneGrd)\n",
    "\n",
    "                Ngrd = paddle.stack(batch_grd, axis=0)\n",
    "                # print(len(Nimg))\n",
    "                # print(type(Nimg))\n",
    "                # print(Ngrd.shape)\n",
    "                \n",
    "                # 至此便得到一个批量的《图片,梯度》对训练集，即 Nimg => x_data\n",
    "                # =================================================================================================\n",
    "                # 以下开始训练重构网络\n",
    "\n",
    "                img1,img2 = attack(Ngrd)\n",
    "\n",
    "                total_loss = F.mse_loss(img1, x_data) + F.mse_loss(img2, x_data)\n",
    " \n",
    "                # avg_loss.backward()\n",
    "                # opt.step()\n",
    "                # opt.clear_grad()\n",
    "\n",
    "                total_loss.backward()\n",
    "                opt.minimize(total_loss)\n",
    "                attack.clear_gradients()\n",
    "\n",
    "                if batch_id % 100 == 0:\n",
    "                    print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, total_loss.numpy()))\n",
    "\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                save_dygraph(attack.state_dict(),os.path.join(savePath,modelName+\"0605\"))\n",
    "\n",
    "            for img in img2.numpy():\n",
    "                img = img.transpose(1,2,0)\n",
    "                plt.imshow(img)\n",
    "                plt.show()\n",
    "                break;\n",
    "\n",
    "        # 模型保存\n",
    "        # save_dygraph(opt.state_dict(), os.path.join(savePath,modelName))\n",
    "        save_dygraph(attack.state_dict(),os.path.join(savePath,modelName+\"0605\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 加载模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, Mapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sized\n"
     ]
    }
   ],
   "source": [
    "import os, cv2\n",
    "from paddle.fluid.dygraph import to_variable, save_dygraph\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image  \n",
    "\n",
    "def test2(use_gpu = True,sample = \"random\" ,dataSet=\"MNIST\",meath=\"Mosaic\",inputs=5000,classid=5):\n",
    "\n",
    "    if sample != \"random\":\n",
    "        data_index = np.loadtxt(\"maxIndex\"+str(2)+\".txt\", delimiter=',').astype('int32')\n",
    "        data_index = paddle.to_tensor(data_index)\n",
    "    else:\n",
    "        data_index = np.loadtxt(\"randomIndex\"+str(2)+\".txt\", delimiter=',').astype('int32')\n",
    "        data_index = paddle.to_tensor(data_index)\n",
    "\n",
    "    # 使用GPU\n",
    "    place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()\n",
    "\n",
    "    with fluid.dygraph.guard(place):\n",
    "        # 实例化模型\n",
    "        attack1 = FCLAndAutoencoder(inputs = inputs)\n",
    "        modelName = \"FCLAndAutoencoder\"\n",
    "        savePath = os.path.join(dataSet+\"_\"+modelName,meath)\n",
    "        model_para, _ = fluid.load_dygraph(savePath + \"/\"+modelName+str(classid))\n",
    "        attack1.load_dict(model_para)\n",
    "        attack1.eval()\n",
    "\n",
    "        attack2 = FCLAndUNet(inputs = inputs)\n",
    "        modelName = \"FCLAndUNet\"\n",
    "        savePath = os.path.join(dataSet+\"_\"+modelName,meath)\n",
    "        model_para, _ = fluid.load_dygraph(savePath + \"/\"+modelName+str(classid))\n",
    "        attack2.load_dict(model_para)\n",
    "        attack2.eval()\n",
    "\n",
    "        attack3 = FCLAndMerge(inputs = inputs,a = 0.3, b =0.7)\n",
    "        modelName = \"merge\"\n",
    "        savePath = os.path.join(dataSet+\"_\"+modelName,meath)\n",
    "        model_para, _ = fluid.load_dygraph(savePath + \"/\"+modelName+str(classid))\n",
    "        attack3.load_dict(model_para)\n",
    "        attack3.eval()\n",
    "\n",
    "        model = MyNet(num_classes=10)\n",
    "        model_para, _ = fluid.load_dygraph('Cifar10_MyNet/no/MyNet')\n",
    "        model.load_dict(model_para)\n",
    "        model.eval()\n",
    "\n",
    "        re_model = MyVGG16()\n",
    "        model_para, _ = fluid.load_dygraph('Cifar10_vgg16/no/vgg16')\n",
    "\n",
    "        # re_model = MyVGG16()\n",
    "        # model_para, _ = fluid.load_dygraph('Cifar10_MyNet/no/MyNet.pdparams')\n",
    "        re_model.load_dict(model_para)\n",
    "        re_model.eval()\n",
    "\n",
    "        print('Start testing...')\n",
    "\n",
    "        cnt = 0\n",
    "        pnr1 = []\n",
    "        pnr2 = []\n",
    "        pnr3 = []\n",
    "\n",
    "        acc = []\n",
    "        acc1 = []\n",
    "        acc2 = []\n",
    "        acc3 = []\n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            # 一个批量的图片[64,3,32,32]\n",
    "            x_data = data[0]\n",
    "            # 一个批量的标签[64]\n",
    "            y_data = data[1]\n",
    "\n",
    "            b,c,w,h = x_data.shape\n",
    "\n",
    "            # 将y再包上一层[[64]]\n",
    "            # y_data = paddle.to_tensor(data[1])\n",
    "            # y_data = paddle.unsqueeze(y_data, 1)\n",
    "        \n",
    "            batch_grd = []\n",
    "            # 遍历一个批量,计算单张图片对应的梯度\n",
    "            for img,lab in zip(x_data.numpy(),y_data.numpy()):\n",
    "                # 单张图片也表示成批量的形式\n",
    "                image = paddle.to_tensor(np.expand_dims(img,axis = 0))\n",
    "                lable = paddle.to_tensor(np.expand_dims(lab,axis = 0))\n",
    "    \n",
    "                # compute original gradient\n",
    "                pre = model(image)\n",
    "                \n",
    "                avg_loss = F.cross_entropy(input=pre, label=lable)\n",
    "\n",
    "                # avg_loss.backward()\n",
    "\n",
    "                # dy_dx_list = []\n",
    "                # for value in model.parameters():\n",
    "                #     dy_dx_list.append(value.grad.detach().flatten())\n",
    "                # model.clear_gradients()\n",
    "\n",
    "                dy_dx = paddle.grad(avg_loss, model.parameters())\n",
    "\n",
    "                # 梯度+偏置的组合\n",
    "                dy_dx_list = [ paddle.flatten(x) for i,x in enumerate(dy_dx)]\n",
    "\n",
    "                oneGrd = paddle.concat(dy_dx_list)\n",
    "                model.clear_gradients()\n",
    "                # print(oneGrd.shape)\n",
    "                \n",
    "                if sample != \"random\":\n",
    "                    # 降序排序\n",
    "                    temp_list = []\n",
    "                    oneGrd = paddle.sort(oneGrd,descending = True)\n",
    "                    temp_list.append(oneGrd[:2500])\n",
    "                    temp_list.append(oneGrd[-2500:])\n",
    "                    oneGrd = paddle.concat(temp_list)\n",
    "                \n",
    "                oneGrd = paddle.index_select(x=oneGrd, index=data_index)\n",
    "                # print(len(dy_dx_list))\n",
    "                # print(oneGrd.shape)\n",
    "\n",
    "                batch_grd.append(oneGrd)\n",
    "\n",
    "            Ngrd = paddle.stack(batch_grd, axis=0)\n",
    "            \n",
    "            # 至此便得到一个批量的图片对训练集，即 Nimg => x_data\n",
    "            # =================================================================================================\n",
    "            # 以下开始重构\n",
    "            y_data = paddle.to_tensor(data[1])\n",
    "            y_data = paddle.unsqueeze(y_data, 1)\n",
    "\n",
    "            _,re_imag1 = attack1(Ngrd)\n",
    "            _,re_imag2 = attack2(Ngrd)\n",
    "            _,re_imag3 = attack3(Ngrd)\n",
    "\n",
    "            re_imag1 = paddle.reshape(re_imag1,[b,c,w,h])\n",
    "            re_imag2 = paddle.reshape(re_imag2,[b,c,w,h])\n",
    "            re_imag3 = paddle.reshape(re_imag3,[b,c,w,h])\n",
    "            \n",
    "            _,pre = re_model(x_data)\n",
    "            _,pre1 = re_model(re_imag1)\n",
    "            _,pre2 = re_model(re_imag2)\n",
    "            _,pre3 = re_model(re_imag3)\n",
    "\n",
    "            acc.append(paddle.metric.accuracy(pre,y_data).numpy())\n",
    "\n",
    "            acc1.append(paddle.metric.accuracy(pre1,y_data).numpy())\n",
    "            acc2.append(paddle.metric.accuracy(pre2,y_data).numpy())\n",
    "            acc3.append(paddle.metric.accuracy(pre3,y_data).numpy())\n",
    "\n",
    "            pnr1.append(get_psnr3(x_data.numpy(),re_imag1.numpy()))\n",
    "            pnr2.append(get_psnr3(x_data.numpy(),re_imag2.numpy()))\n",
    "            pnr3.append(get_psnr3(x_data.numpy(),re_imag3.numpy()))\n",
    "\n",
    "        print(\"acc = \" + str(np.mean(acc)))\n",
    "        \n",
    "        print(\"acc1: {}, acc2: {}, acc3: {}\".format(np.mean(acc1), np.mean(acc2), np.mean(acc3)))\n",
    "        print(\"pnr1: {}, pnr2: {}, pnr3: {}\".format(np.mean(pnr1), np.mean(pnr2), np.mean(pnr3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing...\n",
      "acc = 0.9482422\n",
      "acc1: 0.037109375, acc2: 0.0556640625, acc3: 0.0810546875\n",
      "pnr1: 13.647955367368256, pnr2: 13.536643849247605, pnr3: 13.400713417592035\n"
     ]
    }
   ],
   "source": [
    "test2(sample = \"random\",dataSet=\"Cifar10\",meath=\"class_lable\",inputs=5000,classid=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing...\n",
      "acc = 0.7861328\n",
      "acc1: 0.19140625, acc2: 0.2646484375, acc3: 0.2666015625\n",
      "pnr1: 14.964776999312459, pnr2: 14.837088747258717, pnr3: 14.454131766168171\n"
     ]
    }
   ],
   "source": [
    "test2(sample = \"random\",dataSet=\"Cifar10\",meath=\"class_lable\",inputs=5000,classid=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing...\n",
      "acc = 0.7109375\n",
      "acc1: 0.6796875, acc2: 0.671875, acc3: 0.748046875\n",
      "pnr1: 14.459201672531172, pnr2: 14.36910019615776, pnr3: 14.254182720099804\n"
     ]
    }
   ],
   "source": [
    "test2(sample = \"random\",dataSet=\"Cifar10\",meath=\"class_lable\",inputs=5000,classid=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "from paddle.fluid.dygraph import to_variable, save_dygraph\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image  \n",
    "\n",
    "def test3(use_gpu = True,sample = \"random\" ,dataSet=\"MNIST\",meath=\"Mosaic\",inputs=5000,classid = 5):\n",
    "\n",
    "    if sample != \"random\":\n",
    "        data_index = np.loadtxt(\"maxIndex\"+str(2)+\".txt\", delimiter=',').astype('int32')\n",
    "        data_index = paddle.to_tensor(data_index)\n",
    "    else:\n",
    "        data_index = np.loadtxt(\"randomIndex\"+str(2)+\".txt\", delimiter=',').astype('int32')\n",
    "        data_index = paddle.to_tensor(data_index)\n",
    "\n",
    "    # 使用GPU\n",
    "    place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()\n",
    "\n",
    "    with fluid.dygraph.guard(place):\n",
    "        # 实例化模型\n",
    "        attack1 = FCLAndAutoencoder(inputs = inputs)\n",
    "        modelName = \"FCLAndAutoencoder\"\n",
    "        savePath = os.path.join(dataSet+\"_\"+modelName,meath)\n",
    "        model_para, _ = fluid.load_dygraph(savePath + \"/\"+modelName+str(classid))\n",
    "        attack1.load_dict(model_para)\n",
    "        attack1.eval()\n",
    "\n",
    "        attack2 = FCLAndUNet(inputs = inputs)\n",
    "        modelName = \"FCLAndUNet\"\n",
    "        savePath = os.path.join(dataSet+\"_\"+modelName,meath)\n",
    "        model_para, _ = fluid.load_dygraph(savePath + \"/\"+modelName+str(classid))\n",
    "        attack2.load_dict(model_para)\n",
    "        attack2.eval()\n",
    "\n",
    "        attack3 = FCLAndMerge(inputs = inputs,a = 0.3, b =0.7)\n",
    "        modelName = \"merge\"\n",
    "        savePath = os.path.join(dataSet+\"_\"+modelName,meath)\n",
    "        model_para, _ = fluid.load_dygraph(savePath + \"/\"+modelName+str(classid))\n",
    "        attack3.load_dict(model_para)\n",
    "        attack3.eval()\n",
    "\n",
    "        model = MyNet(num_classes=10)\n",
    "        model_para, _ = fluid.load_dygraph('Cifar10_MyNet/no/MyNet')\n",
    "        model.load_dict(model_para)\n",
    "        model.eval()\n",
    "\n",
    "        print('Start testing...')\n",
    "\n",
    "        cnt = 0\n",
    "        pnr1 = []\n",
    "        pnr2 = []\n",
    "        pnr3 = []\n",
    "\n",
    "        acc = []\n",
    "        acc1 = []\n",
    "        acc2 = []\n",
    "        acc3 = []\n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            # 一个批量的图片[64,3,32,32]\n",
    "            x_data = data[0]\n",
    "            # 一个批量的标签[64]\n",
    "            y_data = data[1]\n",
    "\n",
    "            b,c,w,h = x_data.shape\n",
    "\n",
    "            # 将y再包上一层[[64]]\n",
    "            # y_data = paddle.to_tensor(data[1])\n",
    "            # y_data = paddle.unsqueeze(y_data, 1)\n",
    "        \n",
    "            batch_grd = []\n",
    "            # 遍历一个批量,计算单张图片对应的梯度\n",
    "            for img,lab in zip(x_data.numpy(),y_data.numpy()):\n",
    "                # 单张图片也表示成批量的形式\n",
    "                image = paddle.to_tensor(np.expand_dims(img,axis = 0))\n",
    "                lable = paddle.to_tensor(np.expand_dims(lab,axis = 0))\n",
    "    \n",
    "                # compute original gradient\n",
    "                pre = model(image)\n",
    "                \n",
    "                avg_loss = F.cross_entropy(input=pre, label=lable)\n",
    "\n",
    "                # avg_loss.backward()\n",
    "\n",
    "                # dy_dx_list = []\n",
    "                # for value in model.parameters():\n",
    "                #     dy_dx_list.append(value.grad.detach().flatten())\n",
    "                # model.clear_gradients()\n",
    "\n",
    "                dy_dx = paddle.grad(avg_loss, model.parameters())\n",
    "\n",
    "                # 梯度+偏置的组合\n",
    "                dy_dx_list = [ paddle.flatten(x) for i,x in enumerate(dy_dx)]\n",
    "\n",
    "                oneGrd = paddle.concat(dy_dx_list)\n",
    "                model.clear_gradients()\n",
    "                # print(oneGrd.shape)\n",
    "                \n",
    "                if sample != \"random\":\n",
    "                    # 降序排序\n",
    "                    temp_list = []\n",
    "                    oneGrd = paddle.sort(oneGrd,descending = True)\n",
    "                    temp_list.append(oneGrd[:2500])\n",
    "                    temp_list.append(oneGrd[-2500:])\n",
    "                    oneGrd = paddle.concat(temp_list)\n",
    "                \n",
    "                oneGrd = paddle.index_select(x=oneGrd, index=data_index)\n",
    "                # print(len(dy_dx_list))\n",
    "                # print(oneGrd.shape)\n",
    "\n",
    "                batch_grd.append(oneGrd)\n",
    "\n",
    "            Ngrd = paddle.stack(batch_grd, axis=0)\n",
    "            \n",
    "            # 至此便得到一个批量的图片对训练集，即 Nimg => x_data\n",
    "            # =================================================================================================\n",
    "            # 以下开始重构\n",
    "            y_data = paddle.to_tensor(data[1])\n",
    "            y_data = paddle.unsqueeze(y_data, 1)\n",
    "\n",
    "            _,re_imag1 = attack1(Ngrd)\n",
    "            _,re_imag2 = attack2(Ngrd)\n",
    "            _,re_imag3 = attack3(Ngrd)\n",
    "\n",
    "            re_imag1 = paddle.reshape(re_imag1,[b,c,w,h])\n",
    "            re_imag2 = paddle.reshape(re_imag2,[b,c,w,h])\n",
    "            re_imag3 = paddle.reshape(re_imag3,[b,c,w,h])\n",
    "            \n",
    "            pre = model(x_data)\n",
    "            pre1 = model(re_imag1)\n",
    "            pre2 = model(re_imag2)\n",
    "            pre3 = model(re_imag3)\n",
    "\n",
    "            acc.append(paddle.metric.accuracy(pre,y_data).numpy())\n",
    "\n",
    "            acc1.append(paddle.metric.accuracy(pre1,y_data).numpy())\n",
    "            acc2.append(paddle.metric.accuracy(pre2,y_data).numpy())\n",
    "            acc3.append(paddle.metric.accuracy(pre3,y_data).numpy())\n",
    "\n",
    "            pnr1.append(get_psnr3(x_data.numpy(),re_imag1.numpy()))\n",
    "            pnr2.append(get_psnr3(x_data.numpy(),re_imag2.numpy()))\n",
    "            pnr3.append(get_psnr3(x_data.numpy(),re_imag3.numpy()))\n",
    "\n",
    "        print(\"acc = \" + str(np.mean(acc)))\n",
    "        \n",
    "        print(\"acc1: {}, acc2: {}, acc3: {}\".format(np.mean(acc1), np.mean(acc2), np.mean(acc3)))\n",
    "        print(\"pnr1: {}, pnr2: {}, pnr3: {}\".format(np.mean(pnr1), np.mean(pnr2), np.mean(pnr3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing...\n",
      "acc = 0.6152344\n",
      "acc1: 0.3271484375, acc2: 0.3671875, acc3: 0.3486328125\n",
      "pnr1: 14.925423990813316, pnr2: 14.791677828298914, pnr3: 14.421303984822222\n"
     ]
    }
   ],
   "source": [
    "test3(sample = \"random\",dataSet=\"Cifar10\",meath=\"class_lable\",inputs=5000,classid=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing...\n",
      "acc = 0.52734375\n",
      "acc1: 0.6552734375, acc2: 0.623046875, acc3: 0.708984375\n",
      "pnr1: 14.487584189892287, pnr2: 14.394884990213693, pnr3: 14.257899653747103\n"
     ]
    }
   ],
   "source": [
    "test3(sample = \"random\",dataSet=\"Cifar10\",meath=\"class_lable\",inputs=5000,classid=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing...\n",
      "acc = 0.81347656\n",
      "acc1: 0.11328125, acc2: 0.0771484375, acc3: 0.1376953125\n",
      "pnr1: 13.662036142478547, pnr2: 13.556886658523478, pnr3: 13.410102324322715\n"
     ]
    }
   ],
   "source": [
    "test3(sample = \"random\",dataSet=\"Cifar10\",meath=\"class_lable\",inputs=5000,classid=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadImg():\n",
    "    images = []\n",
    "    labels = []\n",
    "    for lable in range(10):\n",
    "        for i in range(2):\n",
    "            tep =  np.zeros((3,32,32))#创建一个三维的数组\n",
    "            for k in range(3):\n",
    "                tep[k,:,:] = np.loadtxt(\"imgtxt/class\"+str(lable)+\"_\"+str(i)+str(k)+\".txt\",dtype=\"float\",delimiter=\" \")\n",
    "            # plt.imshow(tep.transpose(1,2,0))\n",
    "            # plt.show()\n",
    "            labels.append(lable)\n",
    "            images.append(tep)\n",
    "    return np.array(images).reshape(-1,3,32,32).transpose(0,2,3,1),np.array(labels).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(20, 32, 32, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "(20, 1)\n"
     ]
    }
   ],
   "source": [
    "import paddle # 导入paddle库\n",
    "import paddle.fluid as fluid\n",
    "import numpy as np\n",
    "\n",
    "images,labels = loadImg()\n",
    "\n",
    "plt.imshow(images[0])\n",
    "plt.show()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "\n",
    "print(type(labels))\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "from paddle.fluid.dygraph import to_variable, save_dygraph\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image  \n",
    "\n",
    "def test(use_gpu = True,sample = \"random\" ,dataSet=\"MNIST\",meath=\"Mosaic\",inputs=5000,classid = 5):\n",
    "\n",
    "    if sample != \"random\":\n",
    "        data_index = np.loadtxt(\"maxIndex\"+str(2)+\".txt\", delimiter=',').astype('int32')\n",
    "        data_index = paddle.to_tensor(data_index)\n",
    "    else:\n",
    "        data_index = np.loadtxt(\"randomIndex\"+str(2)+\".txt\", delimiter=',').astype('int32')\n",
    "        data_index = paddle.to_tensor(data_index)\n",
    "\n",
    "    # 使用GPU\n",
    "    place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()\n",
    "\n",
    "    with fluid.dygraph.guard(place):\n",
    "        # 实例化模型\n",
    "        attack1 = FCLAndAutoencoder(inputs = inputs)\n",
    "        modelName = \"FCLAndAutoencoder\"\n",
    "        savePath = os.path.join(dataSet+\"_\"+modelName,meath)\n",
    "        model_para, _ = fluid.load_dygraph(savePath + \"/\"+modelName+str(classid))\n",
    "        attack1.load_dict(model_para)\n",
    "        attack1.eval()\n",
    "\n",
    "        attack2 = FCLAndUNet(inputs = inputs)\n",
    "        modelName = \"FCLAndUNet\"\n",
    "        savePath = os.path.join(dataSet+\"_\"+modelName,meath)\n",
    "        model_para, _ = fluid.load_dygraph(savePath + \"/\"+modelName+str(classid))\n",
    "        attack2.load_dict(model_para)\n",
    "        attack2.eval()\n",
    "\n",
    "        attack3 = FCLAndMerge(inputs = inputs,a = 0.3, b =0.7)\n",
    "        modelName = \"merge\"\n",
    "        savePath = os.path.join(dataSet+\"_\"+modelName,meath)\n",
    "        model_para, _ = fluid.load_dygraph(savePath + \"/\"+modelName+str(classid))\n",
    "        attack3.load_dict(model_para)\n",
    "        attack3.eval()\n",
    "\n",
    "        model = MyNet(num_classes=10)\n",
    "        model_para, _ = fluid.load_dygraph('Cifar10_MyNet/no/MyNet')\n",
    "        model.load_dict(model_para)\n",
    "        model.eval()\n",
    "\n",
    "        print('Start testing...')\n",
    "\n",
    "        cnt = 0\n",
    "        real_imgs = paddle.to_tensor(images.astype(\"float32\"))\n",
    "        real_imgs = paddle.reshape(real_imgs,[-1,1,28,28])\n",
    "\n",
    "        labels = paddle.to_tensor(labels.astype(\"float32\"))\n",
    "   \n",
    "        gen_imgs = attack(real_imgs2)\n",
    "\n",
    "        gen_imgs = gen_imgs.reshape((-1,28,28))\n",
    "\n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            # 一个批量的图片[64,3,32,32]\n",
    "            x_data = data[0]\n",
    "            # 一个批量的标签[64]\n",
    "            y_data = data[1]\n",
    "\n",
    "            b,c,w,h = x_data.shape\n",
    "\n",
    "            # 将y再包上一层[[64]]\n",
    "            # y_data = paddle.to_tensor(data[1])\n",
    "            # y_data = paddle.unsqueeze(y_data, 1)\n",
    "        \n",
    "            batch_grd = []\n",
    "            # 遍历一个批量,计算单张图片对应的梯度\n",
    "            for img,lab in zip(x_data.numpy(),y_data.numpy()):\n",
    "                # 单张图片也表示成批量的形式\n",
    "                image = paddle.to_tensor(np.expand_dims(img,axis = 0))\n",
    "                lable = paddle.to_tensor(np.expand_dims(lab,axis = 0))\n",
    "    \n",
    "                # compute original gradient\n",
    "                pre = model(image)\n",
    "                \n",
    "                avg_loss = F.cross_entropy(input=pre, label=lable)\n",
    "\n",
    "                # avg_loss.backward()\n",
    "\n",
    "                # dy_dx_list = []\n",
    "                # for value in model.parameters():\n",
    "                #     dy_dx_list.append(value.grad.detach().flatten())\n",
    "                # model.clear_gradients()\n",
    "\n",
    "                dy_dx = paddle.grad(avg_loss, model.parameters())\n",
    "\n",
    "                # 梯度+偏置的组合\n",
    "                dy_dx_list = [ paddle.flatten(x) for i,x in enumerate(dy_dx)]\n",
    "\n",
    "                oneGrd = paddle.concat(dy_dx_list)\n",
    "                model.clear_gradients()\n",
    "                # print(oneGrd.shape)\n",
    "                \n",
    "                if sample != \"random\":\n",
    "                    # 降序排序\n",
    "                    temp_list = []\n",
    "                    oneGrd = paddle.sort(oneGrd,descending = True)\n",
    "                    temp_list.append(oneGrd[:2500])\n",
    "                    temp_list.append(oneGrd[-2500:])\n",
    "                    oneGrd = paddle.concat(temp_list)\n",
    "                \n",
    "                oneGrd = paddle.index_select(x=oneGrd, index=data_index)\n",
    "                # print(len(dy_dx_list))\n",
    "                # print(oneGrd.shape)\n",
    "\n",
    "                batch_grd.append(oneGrd)\n",
    "\n",
    "            Ngrd = paddle.stack(batch_grd, axis=0)\n",
    "            \n",
    "            # 至此便得到一个批量的图片对训练集，即 Nimg => x_data\n",
    "            # =================================================================================================\n",
    "            # 以下开始重构\n",
    "\n",
    "            _,re_imag1 = attack1(Ngrd)\n",
    "            _,re_imag2 = attack2(Ngrd)\n",
    "            _,re_imag3 = attack3(Ngrd)\n",
    "\n",
    "            re_imag1 = paddle.reshape(re_imag1,[b,c,w,h])\n",
    "            re_imag2 = paddle.reshape(re_imag2,[b,c,w,h])\n",
    "            re_imag3 = paddle.reshape(re_imag3,[b,c,w,h])\n",
    "            \n",
    "            for i in range(20):\n",
    "                savePath = 'img1/gen_images_'+str(cnt)+'.png'\n",
    "                cnt = cnt + 1\n",
    "                # savePath = \"\"\n",
    "                image_grid(x_data.numpy().transpose(0,2,3,1)[i],1,4,1,save =savePath)\n",
    "                image_grid(re_imag1.numpy().transpose(0,2,3,1)[i],1,4,2,save =savePath)\n",
    "                image_grid(re_imag2.numpy().transpose(0,2,3,1)[i],1,4,3,save =savePath)\n",
    "                image_grid(re_imag3.numpy().transpose(0,2,3,1)[i],1,4,4,save =savePath)\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "            if batch_id > 1:\n",
    "                break;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, cv2\n",
    "from paddle.fluid.dygraph import to_variable, save_dygraph\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image  \n",
    "\n",
    "def test(use_gpu = True,sample = \"random\" ,dataSet=\"MNIST\",meath=\"Mosaic\",inputs=5000,classid = 5):\n",
    "\n",
    "    if sample != \"random\":\n",
    "        data_index = np.loadtxt(\"maxIndex\"+str(2)+\".txt\", delimiter=',').astype('int32')\n",
    "        data_index = paddle.to_tensor(data_index)\n",
    "    else:\n",
    "        data_index = np.loadtxt(\"randomIndex\"+str(2)+\".txt\", delimiter=',').astype('int32')\n",
    "        data_index = paddle.to_tensor(data_index)\n",
    "\n",
    "    # 使用GPU\n",
    "    place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()\n",
    "\n",
    "    with fluid.dygraph.guard(place):\n",
    "        # 实例化模型\n",
    "        attack3 = FCLAndMerge(inputs = inputs,a = 0.3, b =0.7)\n",
    "        modelName = \"merge\"\n",
    "        savePath = os.path.join(dataSet+\"_\"+modelName,meath)\n",
    "        model_para, _ = fluid.load_dygraph(savePath + \"/\"+modelName+str(classid))\n",
    "        attack3.load_dict(model_para)\n",
    "        attack3.eval()\n",
    "\n",
    "        model = MyNet(num_classes=10)\n",
    "        model_para, _ = fluid.load_dygraph('Cifar10_MyNet/no/MyNet')\n",
    "        model.load_dict(model_para)\n",
    "        model.eval()\n",
    "\n",
    "        print('Start testing...')\n",
    "\n",
    "        cnt = 0\n",
    "        real_imgs = paddle.to_tensor(images.astype(\"float32\"))\n",
    "\n",
    "        x_data = paddle.transpose(real_imgs,[0,3,1,2])\n",
    "\n",
    "        y_data = paddle.to_tensor(labels.astype(\"int64\"))\n",
    "\n",
    "        b,c,w,h = x_data.shape\n",
    "\n",
    "        # 将y再包上一层[[64]]\n",
    "        # y_data = paddle.to_tensor(data[1])\n",
    "        # y_data = paddle.unsqueeze(y_data, 1)\n",
    "    \n",
    "        batch_grd = []\n",
    "        # 遍历一个批量,计算单张图片对应的梯度\n",
    "        for img,lab in zip(x_data.numpy(),y_data.numpy()):\n",
    "            # 单张图片也表示成批量的形式\n",
    "            image = paddle.to_tensor(np.expand_dims(img,axis = 0))\n",
    "            lable = paddle.to_tensor(np.expand_dims(lab,axis = 0))\n",
    "\n",
    "            # compute original gradient\n",
    "            pre = model(image)\n",
    "            \n",
    "            avg_loss = F.cross_entropy(input=pre, label=lable)\n",
    "\n",
    "            # avg_loss.backward()\n",
    "\n",
    "            # dy_dx_list = []\n",
    "            # for value in model.parameters():\n",
    "            #     dy_dx_list.append(value.grad.detach().flatten())\n",
    "            # model.clear_gradients()\n",
    "\n",
    "            dy_dx = paddle.grad(avg_loss, model.parameters())\n",
    "\n",
    "            # 梯度+偏置的组合\n",
    "            dy_dx_list = [ paddle.flatten(x) for i,x in enumerate(dy_dx)]\n",
    "\n",
    "            oneGrd = paddle.concat(dy_dx_list)\n",
    "            model.clear_gradients()\n",
    "            # print(oneGrd.shape)\n",
    "            \n",
    "            if sample != \"random\":\n",
    "                # 降序排序\n",
    "                temp_list = []\n",
    "                oneGrd = paddle.sort(oneGrd,descending = True)\n",
    "                temp_list.append(oneGrd[:2500])\n",
    "                temp_list.append(oneGrd[-2500:])\n",
    "                oneGrd = paddle.concat(temp_list)\n",
    "            \n",
    "            oneGrd = paddle.index_select(x=oneGrd, index=data_index)\n",
    "            # print(len(dy_dx_list))\n",
    "            # print(oneGrd.shape)\n",
    "\n",
    "            batch_grd.append(oneGrd)\n",
    "\n",
    "        Ngrd = paddle.stack(batch_grd, axis=0)\n",
    "        \n",
    "        # 至此便得到一个批量的图片对训练集，即 Nimg => x_data\n",
    "        # =================================================================================================\n",
    "        # 以下开始重构\n",
    "\n",
    "        _,re_imag3 = attack3(Ngrd)\n",
    "\n",
    "        re_imag3 = paddle.reshape(re_imag3,[b,c,w,h])\n",
    "        \n",
    "        for i in range(20):\n",
    "            # savePath = 'img1/gen_images_'+str(cnt)+'.png'\n",
    "            savePath = \"\"\n",
    "            cnt = cnt + 1\n",
    "            # savePath = \"\"\n",
    "            aa = x_data.numpy().transpose(0,2,3,1)[i]\n",
    "            image_grid(aa,1,3,1,save =savePath)\n",
    "\n",
    "            plt.subplot(1,3,2)\n",
    "            canvas = np.zeros((28, 28), np.uint8)\n",
    "            canvas_1 = 255 - canvas\n",
    "\n",
    "            plt.imshow(canvas_1,cmap = 'gray',  interpolation = 'none', vmin=0, vmax=255)\n",
    "            plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "            \n",
    "            bb = re_imag3.numpy().transpose(0,2,3,1)[i]\n",
    "            image_grid(bb,1,3,3,save =savePath)\n",
    "            \n",
    "            print(\"psnr is {}\".format(psnr(aa,bb,1.0)))\n",
    "            print(\"ssim is {}\".format(ssim(aa,bb,1.0)))\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaa\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "echo(\"aaaaa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing...\n",
      "mse is 0.024197222664952278\n",
      "psnr is 16.162344790817656\n",
      "ssim is 0.22676633183712003\n",
      "mse is 0.12841077148914337\n",
      "psnr is 8.913985447867221\n",
      "ssim is 0.03964905405304754\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test(sample = \"random\",dataSet=\"Cifar10\",meath=\"class_lable\",inputs=5000,classid=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_dummy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_103/4023551440.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 18\u001B[0;31m \u001B[0;32mfor\u001B[0m \u001B[0mimidx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_dummy\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     19\u001B[0m     \u001B[0midx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0midx_shuffle\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mimidx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m     \u001B[0mimidx_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'num_dummy' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#加拉普拉斯噪声，默认为原始数据添加μ为0，b为1的噪声\n",
    "def add_laplace_noise(data_array, μ=0, b=0.01):\n",
    "    # shp = data_array.shape\n",
    "    # temp = data_array.flatten()\n",
    "    # laplace_noise = np.random.laplace(μ, b, len(temp))\n",
    "    # temp += laplace_noise\n",
    "    # data_array = temp.reshape(shp)\n",
    "    # print(data_array)\n",
    "    return data_array\n",
    "\n",
    "def get_psnr2(img1, img2):\n",
    "   mse = np.mean((img1/1.0 - img2/1.0) ** 2 )\n",
    "   if mse < 1.0e-10:\n",
    "      return 100\n",
    "   return 10 * np.math.log10(1.0 / mse)\n",
    "\n",
    "\n",
    "for imidx in range(num_dummy):\n",
    "    idx = idx_shuffle[imidx]\n",
    "    imidx_list.append(idx)\n",
    "    tmp_datum = tt(dst[idx][0]).float().to(device)\n",
    "    tmp_datum = tmp_datum.view(1, *tmp_datum.size())\n",
    "    tmp_label = torch.Tensor([dst[idx][1]]).long().to(device)\n",
    "    tmp_label = tmp_label.view(1, )\n",
    "    if imidx == 0:\n",
    "        gt_data = tmp_datum\n",
    "        gt_label = tmp_label\n",
    "    else:\n",
    "        gt_data = torch.cat((gt_data, tmp_datum), dim=0)\n",
    "        gt_label = torch.cat((gt_label, tmp_label), dim=0)\n",
    "\n",
    "\n",
    "# compute original gradient\n",
    "out = net(gt_data)\n",
    "y = criterion(out, gt_label)\n",
    "dy_dx = torch.autograd.grad(y, net.parameters())\n",
    "original_dy_dx = list((_.detach().clone() for _ in dy_dx))\n",
    "\n",
    "restruct_grad = []\n",
    "for i in range(len(original_dy_dx)):\n",
    "    tem = original_dy_dx[i].cpu().numpy()\n",
    "    # print(tem[:10])\n",
    "    restruct_grad.append(torch.from_numpy(add_laplace_noise(tem)).to(device))\n",
    "\n",
    "# print(type(restruct_grad))\n",
    "# print(len(restruct_grad))\n",
    "\n",
    "# generate dummy data and label\n",
    "dummy_data = torch.randn(gt_data.size()).to(device).requires_grad_(True)\n",
    "dummy_label = torch.randn((gt_data.shape[0], num_classes)).to(device).requires_grad_(True)\n",
    "\n",
    "if method == 'DLG':\n",
    "    optimizer = torch.optim.LBFGS([dummy_data, dummy_label], lr=lr)\n",
    "elif method == 'iDLG':\n",
    "    optimizer = torch.optim.LBFGS([dummy_data, ], lr=lr)\n",
    "    # predict the ground-truth label\n",
    "    label_pred = torch.argmin(torch.sum(restruct_grad[-2], dim=-1), dim=-1).detach().reshape((1,)).requires_grad_(False)\n",
    "\n",
    "\n",
    "if method == 'DLG':\n",
    "    dummy_loss = - torch.mean(torch.sum(torch.softmax(dummy_label, -1) * torch.log(torch.softmax(pred, -1)), dim=-1))\n",
    "    # dummy_loss = criterion(pred, gt_label)\n",
    "elif method == 'iDLG':\n",
    "    dummy_loss = criterion(pred, label_pred)\n",
    "\n",
    "dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
    "\n",
    "grad_diff = 0\n",
    "for gx, gy in zip(dummy_dy_dx, restruct_grad):\n",
    "    grad_diff += ((gx - gy) ** 2).sum()\n",
    "grad_diff.backward()\n",
    "return grad_diff\n",
    "\n",
    "\n",
    "for imidx in range(num_dummy):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(3, 10, 1)\n",
    "    plt.imshow(tp(gt_data[imidx].cpu()))\n",
    "    # plt.show()\n",
    "    for i in range(min(len(history), 29)):\n",
    "        plt.subplot(3, 10, i + 2)\n",
    "        plt.imshow(history[i][imidx])\n",
    "\n",
    "        psnr.append(get_psnr2(dummy_data[imidx].detach().cpu().numpy(),gt_data[imidx].cpu().numpy()))\n",
    "        f = open(\"psnr\"+str(idx_net)+\".txt\", \"w\")\n",
    "        f.writelines(str(psnr))\n",
    "        f.close()\n",
    "\n",
    "        plt.title('iter=%d' % (history_iters[i]))\n",
    "        plt.axis('off')\n",
    "        # plt.show()\n",
    "    if method == 'DLG':\n",
    "        plt.savefig('%s/DLG_on_%s_%05d.png' % (save_path, imidx_list, imidx_list[imidx]))\n",
    "        plt.close()\n",
    "    elif method == 'iDLG':\n",
    "        plt.savefig('%s/iDLG_on_%s_%05d.png' % (save_path, imidx_list, imidx_list[imidx]))\n",
    "        plt.close()\n",
    "\n",
    "if current_loss < 0.000001: # converge\n",
    "    break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
